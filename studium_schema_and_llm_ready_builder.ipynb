{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31da3c78",
   "metadata": {},
   "source": [
    "# Studium Parisiense — Schema Targeting + LLM‑Ready Dataset Builder\n",
    "\n",
    "This notebook does two things:\n",
    "\n",
    "1. **Schema targeting (data‑driven):** scans the full Studium JSONL dataset and enumerates:\n",
    "   - all sections and subfields present\n",
    "   - meta-entity signals (`places`, `institutions`, `names`, `titles`, `dates`, `id`)\n",
    "   - an exhaustive predicate inventory (e.g., `origin.birthPlace`, `professionalCareer.royalAdministration`, …)\n",
    "\n",
    "2. **LLM‑ready dataset generation (one record per person by default):**\n",
    "   - renders each profile into a clean, consistent **text document**\n",
    "   - cleans common markup (`$...$`, `&...&`, `%...%`, `word=word`)\n",
    "   - keeps **structured metadata** (unique places/institutions/names/titles/dates) alongside the text for later linking / evaluation\n",
    "   - optionally **chunks very long profiles** (toggle), but the default is *one record per person*.\n",
    "\n",
    "> **Input:** a JSON Lines file (`.jsonl`) where each line is one profile record.  \n",
    "> **Output:** a JSON Lines file ready to feed into an LLM pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd5616f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config OK\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "INPUT_JSONL = \"studium_parisiense_dataset.jsonl\" \n",
    "BASE_URL = \"http://studium-parisiense.univ-paris1.fr\"\n",
    "\n",
    "OUTPUT_DIR = \"llm_ready_outputs2\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# What to include in the generated text\n",
    "INCLUDE_BIBLIOGRAPHY = True\n",
    "INCLUDE_RAW = False  # raw is very verbose; \n",
    "INCLUDE_REFERENCES = True   # include 'reference' lines inside each section item\n",
    "INCLUDE_COMMENTS = True     # include 'comment' lines inside each section item\n",
    "\n",
    "# Text size policy\n",
    "ONE_RECORD_PER_PERSON = True \n",
    "CHUNK_LONG_RECORDS = False    # if True, long records are split into chunks now\n",
    "MAX_CHARS_PER_CHUNK = 12000   # used only if CHUNK_LONG_RECORDS=True\n",
    "\n",
    "# If ONE_RECORD_PER_PERSON=True and CHUNK_LONG_RECORDS=False:\n",
    "# we still flag very long profiles for future chunking.\n",
    "WARN_IF_TEXT_GT = 20000\n",
    "\n",
    "# Output files\n",
    "OUT_LLM_READY_JSONL = os.path.join(OUTPUT_DIR, \"studium_llm_ready_people.jsonl\")\n",
    "OUT_SCHEMA_PREDICATES_CSV = os.path.join(OUTPUT_DIR, \"schema_predicates_inventory.csv\")\n",
    "OUT_SECTIONS_CSV = os.path.join(OUTPUT_DIR, \"schema_sections_coverage.csv\")\n",
    "OUT_META_KEYS_CSV = os.path.join(OUTPUT_DIR, \"schema_meta_keys.csv\")\n",
    "OUT_LONG_RECORDS_CSV = os.path.join(OUTPUT_DIR, \"very_long_profiles_to_chunk_later.csv\")\n",
    "\n",
    "print(\"Config OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8edb34c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers loaded\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "\n",
    "def read_jsonl(path: str) -> Iterable[Dict[str, Any]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_no, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                raise ValueError(f\"Invalid JSON on line {line_no}: {e}\") from e\n",
    "\n",
    "\n",
    "def ensure_list(x: Any) -> List[Any]:\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    return [x]\n",
    "\n",
    "\n",
    "def normalize_markup(text: str) -> str:\n",
    "    \"\"\"Best-effort cleanup of Studium inline markup.\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    s = str(text)\n",
    "\n",
    "    # Replace '=' between word characters with a space: \"Città=del=Vaticano\" -> \"Città del Vaticano\"\n",
    "    s = re.sub(r\"(?<=\\w)=(?=\\w)\", \" \", s)\n",
    "\n",
    "    # Remove markup delimiters (keep content)\n",
    "    s = s.replace(\"$\", \"\")\n",
    "    s = s.replace(\"&\", \"\")\n",
    "    s = s.replace(\"%\", \"\")\n",
    "\n",
    "    # Common stray pattern after removing % (e.g., ':1485' used as '%:1485%')\n",
    "    s = re.sub(r\"\\b:\\s*(\\d{3,4})\\b\", r\"\\1\", s)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    s = s.replace(\"\\t\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def safe_get_value(item: Any) -> str:\n",
    "    if item is None:\n",
    "        return \"\"\n",
    "    if isinstance(item, dict):\n",
    "        return normalize_markup(item.get(\"value\", \"\"))\n",
    "    return normalize_markup(str(item))\n",
    "\n",
    "\n",
    "def iter_fact_items(section_value: Any) -> Iterable[Tuple[str, Dict[str, Any]]]:\n",
    "    if not isinstance(section_value, dict):\n",
    "        return\n",
    "    for subfield, arr in section_value.items():\n",
    "        for item in ensure_list(arr):\n",
    "            if isinstance(item, dict):\n",
    "                yield subfield, item\n",
    "            else:\n",
    "                yield subfield, {\"value\": str(item), \"meta\": {}}\n",
    "\n",
    "\n",
    "def extract_meta_entities_from_item(item: Dict[str, Any]) -> Dict[str, List[Any]]:\n",
    "    meta = item.get(\"meta\")\n",
    "    if not isinstance(meta, dict):\n",
    "        return {}\n",
    "    return {k: ensure_list(v) for k, v in meta.items()}\n",
    "\n",
    "\n",
    "def merge_entity_sets(dst: Dict[str, set], src: Dict[str, List[Any]]) -> None:\n",
    "    for k, vals in src.items():\n",
    "        if k not in dst:\n",
    "            dst[k] = set()\n",
    "        for v in vals:\n",
    "            if isinstance(v, dict):\n",
    "                dst[k].add(json.dumps(v, ensure_ascii=False, sort_keys=True))\n",
    "            else:\n",
    "                dst[k].add(str(v))\n",
    "\n",
    "\n",
    "def canonical_person_name(rec: Dict[str, Any]) -> str:\n",
    "    ident = rec.get(\"identity\", {})\n",
    "    if isinstance(ident, dict) and \"name\" in ident:\n",
    "        name_items = ensure_list(ident.get(\"name\"))\n",
    "        if name_items:\n",
    "            v = safe_get_value(name_items[0])\n",
    "            if v:\n",
    "                return v\n",
    "    title = rec.get(\"title\")\n",
    "    return normalize_markup(title) if title else \"\"\n",
    "\n",
    "\n",
    "def absolute_link(link: str) -> str:\n",
    "    if not link:\n",
    "        return \"\"\n",
    "    if link.startswith(\"http://\") or link.startswith(\"https://\"):\n",
    "        return link\n",
    "    return BASE_URL.rstrip(\"/\") + \"/\" + link.lstrip(\"/\")\n",
    "\n",
    "\n",
    "def get_activity_mediane(rec: Dict[str, Any]) -> Optional[int]:\n",
    "    # sometimes in extras, sometimes in identity\n",
    "    extras = rec.get(\"extras\")\n",
    "    if isinstance(extras, dict) and isinstance(extras.get(\"activityMediane\"), int):\n",
    "        return extras[\"activityMediane\"]\n",
    "    ident = rec.get(\"identity\")\n",
    "    if isinstance(ident, dict) and isinstance(ident.get(\"activityMediane\"), list):\n",
    "        # identity.activityMediane usually appears as list of items with 'value' or a raw int\n",
    "        for it in ident[\"activityMediane\"]:\n",
    "            if isinstance(it, dict):\n",
    "                v = it.get(\"value\")\n",
    "                if isinstance(v, int):\n",
    "                    return v\n",
    "                if isinstance(v, str) and v.strip().isdigit():\n",
    "                    return int(v.strip())\n",
    "            elif isinstance(it, int):\n",
    "                return it\n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"Helpers loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6251af76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records scanned: 20,149\n",
      "Extras keys: {'activityMediane': 20045}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>records_with_section</th>\n",
       "      <th>coverage_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>identity</td>\n",
       "      <td>20149</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bibliography</td>\n",
       "      <td>20127</td>\n",
       "      <td>99.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>curriculum</td>\n",
       "      <td>19614</td>\n",
       "      <td>97.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>origin</td>\n",
       "      <td>11193</td>\n",
       "      <td>55.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ecclesiasticalCareer</td>\n",
       "      <td>10924</td>\n",
       "      <td>54.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>professionalCareer</td>\n",
       "      <td>6740</td>\n",
       "      <td>33.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>relationalInsertion</td>\n",
       "      <td>6257</td>\n",
       "      <td>31.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>textualProduction</td>\n",
       "      <td>1562</td>\n",
       "      <td>7.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>assets</td>\n",
       "      <td>1361</td>\n",
       "      <td>6.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>politicalCareer</td>\n",
       "      <td>1108</td>\n",
       "      <td>5.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>travels</td>\n",
       "      <td>720</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>commissions</td>\n",
       "      <td>223</td>\n",
       "      <td>1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>distinctiveSign</td>\n",
       "      <td>98</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>otherActivities</td>\n",
       "      <td>61</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>orality</td>\n",
       "      <td>51</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 section  records_with_section  coverage_pct\n",
       "0               identity                 20149        100.00\n",
       "1           bibliography                 20127         99.89\n",
       "2             curriculum                 19614         97.34\n",
       "3                 origin                 11193         55.55\n",
       "4   ecclesiasticalCareer                 10924         54.22\n",
       "5     professionalCareer                  6740         33.45\n",
       "6    relationalInsertion                  6257         31.05\n",
       "7      textualProduction                  1562          7.75\n",
       "8                 assets                  1361          6.75\n",
       "9        politicalCareer                  1108          5.50\n",
       "10               travels                   720          3.57\n",
       "11           commissions                   223          1.11\n",
       "12       distinctiveSign                    98          0.49\n",
       "13       otherActivities                    61          0.30\n",
       "14               orality                    51          0.25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meta_key</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dates</td>\n",
       "      <td>115631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>places</td>\n",
       "      <td>63626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>institutions</td>\n",
       "      <td>62969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>names</td>\n",
       "      <td>51789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>titles</td>\n",
       "      <td>25621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>isComment</td>\n",
       "      <td>20153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>isLink</td>\n",
       "      <td>20149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cotes</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       meta_key   count\n",
       "0         dates  115631\n",
       "1        places   63626\n",
       "2  institutions   62969\n",
       "3         names   51789\n",
       "4        titles   25621\n",
       "5     isComment   20153\n",
       "6        isLink   20149\n",
       "7         cotes      52"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================\n",
    "# 1) Schema targeting (data-driven)\n",
    "# =========================\n",
    "\n",
    "TOP_LEVEL_IGNORE = {\"_id\", \"reference\", \"title\", \"link\", \"raw\"}  # we keep 'extras' because it contains activityMediane\n",
    "\n",
    "section_counts = Counter()\n",
    "predicate_counts = Counter()\n",
    "meta_key_counts = Counter()\n",
    "predicate_meta_counts: Dict[str, Counter] = defaultdict(Counter)\n",
    "\n",
    "extras_key_counts = Counter()\n",
    "\n",
    "n_records = 0\n",
    "\n",
    "for rec in read_jsonl(INPUT_JSONL):\n",
    "    n_records += 1\n",
    "\n",
    "    # extras keys\n",
    "    extras = rec.get(\"extras\")\n",
    "    if isinstance(extras, dict):\n",
    "        extras_key_counts.update(extras.keys())\n",
    "\n",
    "    # Which top-level sections exist?\n",
    "    for k in rec.keys():\n",
    "        if k in TOP_LEVEL_IGNORE or k == \"extras\":\n",
    "            continue\n",
    "        section_counts[k] += 1\n",
    "\n",
    "    # Predicates: section.subfield\n",
    "    for section, sec_val in rec.items():\n",
    "        if section in TOP_LEVEL_IGNORE or section == \"extras\":\n",
    "            continue\n",
    "        if not isinstance(sec_val, dict):\n",
    "            continue\n",
    "\n",
    "        for subfield, item in iter_fact_items(sec_val):\n",
    "            pred = f\"{section}.{subfield}\"\n",
    "            predicate_counts[pred] += 1\n",
    "\n",
    "            meta = item.get(\"meta\")\n",
    "            if isinstance(meta, dict):\n",
    "                for mk in meta.keys():\n",
    "                    meta_key_counts[mk] += 1\n",
    "                    predicate_meta_counts[pred][mk] += 1\n",
    "\n",
    "print(f\"Records scanned: {n_records:,}\")\n",
    "print(\"Extras keys:\", dict(extras_key_counts))\n",
    "\n",
    "# Save schema tables\n",
    "df_sections = pd.DataFrame(section_counts.most_common(), columns=[\"section\", \"records_with_section\"])\n",
    "df_sections[\"coverage_pct\"] = (df_sections[\"records_with_section\"] / n_records * 100).round(2)\n",
    "df_sections.to_csv(OUT_SECTIONS_CSV, index=False)\n",
    "\n",
    "rows = []\n",
    "for pred, cnt in predicate_counts.most_common():\n",
    "    meta_keys = dict(predicate_meta_counts[pred].most_common())\n",
    "    rows.append({\n",
    "        \"predicate\": pred,\n",
    "        \"records_with_predicate\": cnt,\n",
    "        \"meta_keys_observed\": json.dumps(meta_keys, ensure_ascii=False),\n",
    "    })\n",
    "df_pred = pd.DataFrame(rows)\n",
    "df_pred.to_csv(OUT_SCHEMA_PREDICATES_CSV, index=False)\n",
    "\n",
    "df_meta = pd.DataFrame(meta_key_counts.most_common(), columns=[\"meta_key\", \"count\"])\n",
    "df_meta.to_csv(OUT_META_KEYS_CSV, index=False)\n",
    "\n",
    "display(df_sections.head(20))\n",
    "display(df_meta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa308b50",
   "metadata": {},
   "source": [
    "## Target schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "660ad44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renderer ready\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 2) Render each person into a LLM-ready text\n",
    "# =========================\n",
    "\n",
    "SECTION_ORDER = [\n",
    "    \"identity\",\n",
    "    \"origin\",\n",
    "    \"curriculum\",\n",
    "    \"ecclesiasticalCareer\",\n",
    "    \"professionalCareer\",\n",
    "    \"relationalInsertion\",\n",
    "    \"textualProduction\",\n",
    "    \"assets\",\n",
    "    \"politicalCareer\",\n",
    "    \"travels\",\n",
    "    \"commissions\",\n",
    "    \"orality\",\n",
    "    \"distinctiveSign\",\n",
    "    \"otherActivities\",\n",
    "    \"bibliography\",\n",
    "]\n",
    "\n",
    "SECTION_TITLES = {\n",
    "    \"identity\": \"IDENTITY\",\n",
    "    \"origin\": \"ORIGIN\",\n",
    "    \"curriculum\": \"CURRICULUM\",\n",
    "    \"ecclesiasticalCareer\": \"ECCLESIASTICAL CAREER\",\n",
    "    \"professionalCareer\": \"PROFESSIONAL CAREER\",\n",
    "    \"relationalInsertion\": \"RELATIONAL INSERTION\",\n",
    "    \"textualProduction\": \"TEXTUAL PRODUCTION\",\n",
    "    \"assets\": \"ASSETS\",\n",
    "    \"politicalCareer\": \"POLITICAL CAREER\",\n",
    "    \"travels\": \"TRAVELS\",\n",
    "    \"commissions\": \"COMMISSIONS\",\n",
    "    \"orality\": \"ORALITY\",\n",
    "    \"distinctiveSign\": \"DISTINCTIVE SIGN\",\n",
    "    \"otherActivities\": \"OTHER ACTIVITIES\",\n",
    "    \"bibliography\": \"BIBLIOGRAPHY\",\n",
    "}\n",
    "\n",
    "\n",
    "def render_item_lines(item: Dict[str, Any]) -> List[str]:\n",
    "    lines: List[str] = []\n",
    "    v = safe_get_value(item)\n",
    "    if v:\n",
    "        lines.append(f\"- {v}\")\n",
    "\n",
    "    if INCLUDE_COMMENTS:\n",
    "        for c in ensure_list(item.get(\"comment\")):\n",
    "            cv = normalize_markup(str(c))\n",
    "            if cv:\n",
    "                lines.append(f\"  • comment: {cv}\")\n",
    "\n",
    "    if INCLUDE_REFERENCES:\n",
    "        for r in ensure_list(item.get(\"reference\")):\n",
    "            rv = normalize_markup(str(r))\n",
    "            if rv:\n",
    "                lines.append(f\"  • source: {rv}\")\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "def render_generic_section(section_value: Dict[str, Any]) -> List[str]:\n",
    "    out: List[str] = []\n",
    "    for subfield in sorted(section_value.keys()):\n",
    "        items = ensure_list(section_value.get(subfield))\n",
    "        if not items:\n",
    "            continue\n",
    "        out.append(f\"{subfield}:\")\n",
    "        for it in items:\n",
    "            if isinstance(it, dict):\n",
    "                out.extend(render_item_lines(it))\n",
    "            else:\n",
    "                out.append(f\"- {normalize_markup(str(it))}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def render_textual_production(tp: Dict[str, Any]) -> List[str]:\n",
    "    out: List[str] = []\n",
    "    for domain in sorted(tp.keys()):\n",
    "        domval = tp[domain]\n",
    "        if not isinstance(domval, dict):\n",
    "            continue\n",
    "\n",
    "        header = normalize_markup(domval.get(\"value\", domain))\n",
    "        out.append(f\"domain: {domain}\")\n",
    "        if header:\n",
    "            out.append(f\"- {header}\")\n",
    "\n",
    "        opus_list = ensure_list(domval.get(\"opus\"))\n",
    "        if opus_list:\n",
    "            out.append(\"  opus:\")\n",
    "        for opus in opus_list:\n",
    "            if not isinstance(opus, dict):\n",
    "                continue\n",
    "            main_title = normalize_markup(str(opus.get(\"mainTitle\") or \"\"))\n",
    "            if main_title:\n",
    "                out.append(f\"  - work: {main_title}\")\n",
    "\n",
    "            for k in sorted(opus.keys()):\n",
    "                if k == \"mainTitle\":\n",
    "                    continue\n",
    "                vals = ensure_list(opus.get(k))\n",
    "                if not vals:\n",
    "                    continue\n",
    "                for x in vals:\n",
    "                    if isinstance(x, dict):\n",
    "                        xv = safe_get_value(x)\n",
    "                        if xv:\n",
    "                            out.append(f\"    • {k}: {xv}\")\n",
    "                    else:\n",
    "                        out.append(f\"    • {k}: {normalize_markup(str(x))}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_llm_text_and_meta(rec: Dict[str, Any]) -> Tuple[str, Dict[str, List[str]]]:\n",
    "    ref = str(rec.get(\"reference\", \"\")).strip()\n",
    "    name = canonical_person_name(rec)\n",
    "    title = normalize_markup(str(rec.get(\"title\", \"\"))) if rec.get(\"title\") else \"\"\n",
    "    link = absolute_link(rec.get(\"link\", \"\"))\n",
    "\n",
    "    activity_mediane = get_activity_mediane(rec)\n",
    "\n",
    "    meta_entities: Dict[str, set] = {}\n",
    "    lines: List[str] = []\n",
    "\n",
    "    # Header (stable)\n",
    "    lines.append(f\"reference: {ref}\")\n",
    "    if name:\n",
    "        lines.append(f\"name: {name}\")\n",
    "    if title and title != name:\n",
    "        lines.append(f\"title: {title}\")\n",
    "    if link:\n",
    "        lines.append(f\"link: {link}\")\n",
    "    if activity_mediane is not None:\n",
    "        lines.append(f\"activityMediane: {activity_mediane}\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    for section in SECTION_ORDER:\n",
    "        if section == \"bibliography\" and not INCLUDE_BIBLIOGRAPHY:\n",
    "            continue\n",
    "\n",
    "        sec_val = rec.get(section)\n",
    "        if sec_val is None or not isinstance(sec_val, dict):\n",
    "            continue\n",
    "\n",
    "        if section == \"textualProduction\":\n",
    "            section_lines = render_textual_production(sec_val)\n",
    "        else:\n",
    "            section_lines = render_generic_section(sec_val)\n",
    "\n",
    "        if not section_lines:\n",
    "            continue\n",
    "\n",
    "        lines.append(f\"[{SECTION_TITLES.get(section, section.upper())}]\")\n",
    "        lines.extend(section_lines)\n",
    "        lines.append(\"\")\n",
    "\n",
    "        # meta extraction from section fact items\n",
    "        for _, item in iter_fact_items(sec_val):\n",
    "            merge_entity_sets(meta_entities, extract_meta_entities_from_item(item))\n",
    "\n",
    "        # meta extraction inside textualProduction opus\n",
    "        if section == \"textualProduction\":\n",
    "            for _, domval in sec_val.items():\n",
    "                if isinstance(domval, dict):\n",
    "                    for opus in ensure_list(domval.get(\"opus\")):\n",
    "                        if isinstance(opus, dict):\n",
    "                            for vv in opus.values():\n",
    "                                for it in ensure_list(vv):\n",
    "                                    if isinstance(it, dict):\n",
    "                                        merge_entity_sets(meta_entities, extract_meta_entities_from_item(it))\n",
    "\n",
    "    meta_serializable: Dict[str, List[str]] = {k: sorted(list(v)) for k, v in meta_entities.items()}\n",
    "    text = \"\\n\".join(lines).strip()\n",
    "    return text, meta_serializable\n",
    "\n",
    "\n",
    "print(\"Renderer ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83b5d20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 20,149 rows to: llm_ready_outputs2\\studium_llm_ready_people.jsonl\n",
      "Flagged 132 very long profiles for future chunking: llm_ready_outputs2\\very_long_profiles_to_chunk_later.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>name</th>\n",
       "      <th>link</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>12000</td>\n",
       "      <td>THOMAS de Aquino</td>\n",
       "      <td>http://studium-parisiense.univ-paris1.fr/indiv...</td>\n",
       "      <td>921477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50875</td>\n",
       "      <td>AEGIDIUS Romanus</td>\n",
       "      <td>http://studium-parisiense.univ-paris1.fr/indiv...</td>\n",
       "      <td>344970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>54530</td>\n",
       "      <td>HUGO de Sancto Victore</td>\n",
       "      <td>http://studium-parisiense.univ-paris1.fr/indiv...</td>\n",
       "      <td>266786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1533</td>\n",
       "      <td>BONAVENTURA de Bagnoregio</td>\n",
       "      <td>http://studium-parisiense.univ-paris1.fr/indiv...</td>\n",
       "      <td>213028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>385</td>\n",
       "      <td>ALBERTUS Magnus</td>\n",
       "      <td>http://studium-parisiense.univ-paris1.fr/indiv...</td>\n",
       "      <td>204686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>58754</td>\n",
       "      <td>ROBERTUS Grosseteste</td>\n",
       "      <td>http://studium-parisiense.univ-paris1.fr/indiv...</td>\n",
       "      <td>174624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>51944</td>\n",
       "      <td>NICOLAUS de Lyra</td>\n",
       "      <td>http://studium-parisiense.univ-paris1.fr/indiv...</td>\n",
       "      <td>156589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>52376</td>\n",
       "      <td>VINCENTIUS Belvacensis</td>\n",
       "      <td>http://studium-parisiense.univ-paris1.fr/indiv...</td>\n",
       "      <td>138567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16804</td>\n",
       "      <td>ARNALDUS de Villanova</td>\n",
       "      <td>http://studium-parisiense.univ-paris1.fr/indiv...</td>\n",
       "      <td>137887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>56805</td>\n",
       "      <td>PETRUS Lombardus</td>\n",
       "      <td>http://studium-parisiense.univ-paris1.fr/indiv...</td>\n",
       "      <td>131912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>23409</td>\n",
       "      <td>GUILLELMUS Peraldus</td>\n",
       "      <td>http://studium-parisiense.univ-paris1.fr/indiv...</td>\n",
       "      <td>129858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>4925</td>\n",
       "      <td>HENRICUS de Langenstein</td>\n",
       "      <td>http://studium-parisiense.univ-paris1.fr/indiv...</td>\n",
       "      <td>128980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2455</td>\n",
       "      <td>GALTERIUS Burley</td>\n",
       "      <td>http://studium-parisiense.univ-paris1.fr/indiv...</td>\n",
       "      <td>125282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>21498</td>\n",
       "      <td>NICOLAUS Oresme</td>\n",
       "      <td>http://studium-parisiense.univ-paris1.fr/indiv...</td>\n",
       "      <td>110374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2353</td>\n",
       "      <td>FRANCISCUS Villon</td>\n",
       "      <td>http://studium-parisiense.univ-paris1.fr/indiv...</td>\n",
       "      <td>102467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>11680</td>\n",
       "      <td>STEPHANUS Langton</td>\n",
       "      <td>http://studium-parisiense.univ-paris1.fr/indiv...</td>\n",
       "      <td>89129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>56834</td>\n",
       "      <td>PETRUS Comestor</td>\n",
       "      <td>http://studium-parisiense.univ-paris1.fr/indiv...</td>\n",
       "      <td>79940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>22171</td>\n",
       "      <td>GALTERIUS de Castellione</td>\n",
       "      <td>http://studium-parisiense.univ-paris1.fr/indiv...</td>\n",
       "      <td>77318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3630</td>\n",
       "      <td>GUILLELMUS de Moerbeke</td>\n",
       "      <td>http://studium-parisiense.univ-paris1.fr/indiv...</td>\n",
       "      <td>76802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>253</td>\n",
       "      <td>ALANUS Chartier</td>\n",
       "      <td>http://studium-parisiense.univ-paris1.fr/indiv...</td>\n",
       "      <td>76665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    reference                       name  \\\n",
       "126     12000           THOMAS de Aquino   \n",
       "10      50875           AEGIDIUS Romanus   \n",
       "77      54530     HUGO de Sancto Victore   \n",
       "17       1533  BONAVENTURA de Bagnoregio   \n",
       "3         385            ALBERTUS Magnus   \n",
       "111     58754       ROBERTUS Grosseteste   \n",
       "92      51944           NICOLAUS de Lyra   \n",
       "131     52376     VINCENTIUS Belvacensis   \n",
       "1       16804      ARNALDUS de Villanova   \n",
       "100     56805           PETRUS Lombardus   \n",
       "46      23409        GUILLELMUS Peraldus   \n",
       "81       4925    HENRICUS de Langenstein   \n",
       "68       2455           GALTERIUS Burley   \n",
       "93      21498            NICOLAUS Oresme   \n",
       "37       2353          FRANCISCUS Villon   \n",
       "122     11680          STEPHANUS Langton   \n",
       "108     56834            PETRUS Comestor   \n",
       "70      22171   GALTERIUS de Castellione   \n",
       "47       3630     GUILLELMUS de Moerbeke   \n",
       "11        253            ALANUS Chartier   \n",
       "\n",
       "                                                  link  text_len  \n",
       "126  http://studium-parisiense.univ-paris1.fr/indiv...    921477  \n",
       "10   http://studium-parisiense.univ-paris1.fr/indiv...    344970  \n",
       "77   http://studium-parisiense.univ-paris1.fr/indiv...    266786  \n",
       "17   http://studium-parisiense.univ-paris1.fr/indiv...    213028  \n",
       "3    http://studium-parisiense.univ-paris1.fr/indiv...    204686  \n",
       "111  http://studium-parisiense.univ-paris1.fr/indiv...    174624  \n",
       "92   http://studium-parisiense.univ-paris1.fr/indiv...    156589  \n",
       "131  http://studium-parisiense.univ-paris1.fr/indiv...    138567  \n",
       "1    http://studium-parisiense.univ-paris1.fr/indiv...    137887  \n",
       "100  http://studium-parisiense.univ-paris1.fr/indiv...    131912  \n",
       "46   http://studium-parisiense.univ-paris1.fr/indiv...    129858  \n",
       "81   http://studium-parisiense.univ-paris1.fr/indiv...    128980  \n",
       "68   http://studium-parisiense.univ-paris1.fr/indiv...    125282  \n",
       "93   http://studium-parisiense.univ-paris1.fr/indiv...    110374  \n",
       "37   http://studium-parisiense.univ-paris1.fr/indiv...    102467  \n",
       "122  http://studium-parisiense.univ-paris1.fr/indiv...     89129  \n",
       "108  http://studium-parisiense.univ-paris1.fr/indiv...     79940  \n",
       "70   http://studium-parisiense.univ-paris1.fr/indiv...     77318  \n",
       "47   http://studium-parisiense.univ-paris1.fr/indiv...     76802  \n",
       "11   http://studium-parisiense.univ-paris1.fr/indiv...     76665  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================\n",
    "# 3) Export LLM-ready JSONL\n",
    "# =========================\n",
    "\n",
    "def chunk_text(text: str, max_chars: int) -> List[str]:\n",
    "    if len(text) <= max_chars:\n",
    "        return [text]\n",
    "    parts = text.split(\"\\n\\n\")\n",
    "    chunks: List[str] = []\n",
    "    cur = \"\"\n",
    "    for p in parts:\n",
    "        if not cur:\n",
    "            cur = p\n",
    "        elif len(cur) + 2 + len(p) <= max_chars:\n",
    "            cur += \"\\n\\n\" + p\n",
    "        else:\n",
    "            chunks.append(cur)\n",
    "            cur = p\n",
    "    if cur:\n",
    "        chunks.append(cur)\n",
    "\n",
    "    final: List[str] = []\n",
    "    for c in chunks:\n",
    "        if len(c) <= max_chars:\n",
    "            final.append(c)\n",
    "        else:\n",
    "            for i in range(0, len(c), max_chars):\n",
    "                final.append(c[i:i+max_chars])\n",
    "    return final\n",
    "\n",
    "\n",
    "n_written = 0\n",
    "long_rows = []\n",
    "\n",
    "with open(OUT_LLM_READY_JSONL, \"w\", encoding=\"utf-8\") as out:\n",
    "    for rec in read_jsonl(INPUT_JSONL):\n",
    "        ref = str(rec.get(\"reference\", \"\")).strip()\n",
    "        name = canonical_person_name(rec)\n",
    "        link = absolute_link(rec.get(\"link\", \"\"))\n",
    "        text, meta_entities = build_llm_text_and_meta(rec)\n",
    "\n",
    "        if ONE_RECORD_PER_PERSON and not CHUNK_LONG_RECORDS:\n",
    "            row = {\n",
    "                \"reference\": ref,\n",
    "                \"name\": name,\n",
    "                \"link\": link,\n",
    "                \"text\": text,\n",
    "                \"meta_entities\": meta_entities,\n",
    "                \"text_len\": len(text),\n",
    "            }\n",
    "            out.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "            n_written += 1\n",
    "\n",
    "            if len(text) > WARN_IF_TEXT_GT:\n",
    "                long_rows.append({\n",
    "                    \"reference\": ref,\n",
    "                    \"name\": name,\n",
    "                    \"link\": link,\n",
    "                    \"text_len\": len(text),\n",
    "                })\n",
    "\n",
    "        else:\n",
    "            chunks = chunk_text(text, MAX_CHARS_PER_CHUNK)\n",
    "            for idx, ch in enumerate(chunks):\n",
    "                row = {\n",
    "                    \"reference\": ref,\n",
    "                    \"name\": name,\n",
    "                    \"link\": link,\n",
    "                    \"chunk_id\": f\"{ref}::chunk{idx:03d}\",\n",
    "                    \"chunk_index\": idx,\n",
    "                    \"n_chunks\": len(chunks),\n",
    "                    \"text\": ch,\n",
    "                    \"meta_entities\": meta_entities,\n",
    "                    \"text_len\": len(ch),\n",
    "                }\n",
    "                out.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "                n_written += 1\n",
    "\n",
    "print(f\"Wrote {n_written:,} rows to: {OUT_LLM_READY_JSONL}\")\n",
    "\n",
    "if long_rows:\n",
    "    df_long = pd.DataFrame(long_rows).sort_values(\"text_len\", ascending=False)\n",
    "    df_long.to_csv(OUT_LONG_RECORDS_CSV, index=False)\n",
    "    print(f\"Flagged {len(long_rows):,} very long profiles for future chunking: {OUT_LONG_RECORDS_CSV}\")\n",
    "    display(df_long.head(20))\n",
    "else:\n",
    "    print(\"No profiles exceeded WARN_IF_TEXT_GT.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b2dfec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique references in output: 20149\n",
      "Text length percentiles: {50: 1236, 75: 1722, 90: 2905, 95: 4476, 99: 13024}\n",
      "Max text_len: 921477\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 4) Quick validation / QA\n",
    "# =========================\n",
    "\n",
    "refs = set()\n",
    "lens = []\n",
    "\n",
    "with open(OUT_LLM_READY_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        row = json.loads(line)\n",
    "        refs.add(row[\"reference\"])\n",
    "        lens.append(row[\"text_len\"])\n",
    "\n",
    "s = pd.Series(lens)\n",
    "print(\"Unique references in output:\", len(refs))\n",
    "print(\"Text length percentiles:\", {p: int(s.quantile(p/100)) for p in [50, 75, 90, 95, 99]})\n",
    "print(\"Max text_len:\", int(s.max()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
