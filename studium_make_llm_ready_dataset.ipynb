{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80150aa6",
   "metadata": {},
   "source": [
    "# Studium Parisiense → Dataset “LLM‑ready” (JSONL chunks)\n",
    "\n",
    "Ce notebook transforme un export **JSON Lines** (1 profil par ligne) issu de Studium Parisiense en un dataset plus simple à donner à un LLM.\n",
    "\n",
    "**Entrée :**\n",
    "- Un fichier `.jsonl` (chaque ligne = 1 fiche)\n",
    "\n",
    "**Sorties (dans `OUTPUT_DIR/`) :**\n",
    "1. `studium_llm_chunks.jsonl`  \n",
    "   → dataset “LLM‑ready” : **1 ligne = 1 chunk de texte** avec métadonnées (`person_reference`, `url`, `doc_type`, etc.)\n",
    "2. `studium_field_paths_coverage.csv`  \n",
    "   → inventaire des attributs (chemins) + fréquence de présence\n",
    "3. `studium_top_level_sections.csv`  \n",
    "   → fréquence des sections top‑level\n",
    "\n",
    "> Pourquoi “chunker” ?  \n",
    "> Certaines fiches (ex. `textualProduction`) peuvent être très longues : chunker évite de dépasser la fenêtre de contexte et facilite un pipeline stable (extraction entités‑relations, RAG, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d1f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & configuration ---\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import Any, Dict, List, Set\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs):\n",
    "        return x\n",
    "\n",
    "INPUT_JSONL = \"studium_parisiense_dataset.jsonl\"\n",
    "\n",
    "BASE_URL = \"http://studium-parisiense.univ-paris1.fr\"\n",
    "\n",
    "OUTPUT_DIR = \"llm_ready_outputs\"\n",
    "\n",
    "# Taille max d'un chunk (caractères)\n",
    "MAX_CHARS = 2500\n",
    "\n",
    "INCLUDE_META = True          # ajoute un mini-résumé des meta (dates/lieux/noms) si dispo\n",
    "INCLUDE_SOURCES = False      # ajoute les champs 'reference' (souvent très longs)\n",
    "\n",
    "# Sections qu'on ignore en général pour le texte LLM (bruit / HTML / redondant)\n",
    "SKIP_TOP_LEVEL_KEYS = {\"_id\", \"raw\", \"extras\"}\n",
    "\n",
    "BIO_SECTION_ORDER = [\n",
    "    \"identity\",\n",
    "    \"origin\",\n",
    "    \"curriculum\",\n",
    "    \"ecclesiasticalCareer\",\n",
    "    \"professionalCareer\",\n",
    "    \"relationalInsertion\",\n",
    "    # d'autres sections peuvent exister : elles seront ajoutées après automatiquement\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d86d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helpers (nettoyage / rendu en texte / inventaire d'attributs) ---\n",
    "\n",
    "def clean_markup(s: str) -> str:\n",
    "    # Nettoie les marqueurs typiques ($...$, %...%, &...&) et espaces\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = s.replace(\"\\t\", \" \").replace(\"\\n\", \" \")\n",
    "    s = s.replace(\"$\", \"\").replace(\"%\", \"\").replace(\"&\", \"\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def summarize_meta(meta: Dict[str, Any]) -> str:\n",
    "    # Résumé compact des meta utiles (dates/places/names/titles) si présentes\n",
    "    if not meta or not isinstance(meta, dict):\n",
    "        return \"\"\n",
    "\n",
    "    parts = []\n",
    "\n",
    "    dates = meta.get(\"dates\")\n",
    "    if isinstance(dates, list) and dates:\n",
    "        def _fmt_date(d: Dict[str, Any]) -> str:\n",
    "            if not isinstance(d, dict):\n",
    "                return \"\"\n",
    "            t = d.get(\"type\")\n",
    "            if t == \"SIMPLE\":\n",
    "                return str(d.get(\"date\", \"\")).strip()\n",
    "            if t == \"BEFORE\":\n",
    "                return f\"≤{d.get('date','')}\"\n",
    "            if t == \"AFTER\":\n",
    "                return f\"≥{d.get('date','')}\"\n",
    "            if t == \"INTERVAL\":\n",
    "                sd = d.get(\"startDate\", {}) or {}\n",
    "                ed = d.get(\"endDate\", {}) or {}\n",
    "                s1 = sd.get(\"date\", \"\")\n",
    "                e1 = ed.get(\"date\", \"\")\n",
    "                if s1 or e1:\n",
    "                    return f\"{s1}-{e1}\"\n",
    "            return \"\"\n",
    "\n",
    "        dstr = \", \".join([x for x in (_fmt_date(x) for x in dates) if x])\n",
    "        if dstr:\n",
    "            parts.append(f\"dates: {dstr}\")\n",
    "\n",
    "    for key, label in [(\"places\", \"lieux\"), (\"names\", \"noms\"), (\"titles\", \"titres\")]:\n",
    "        arr = meta.get(key)\n",
    "        if isinstance(arr, list) and arr:\n",
    "            arr2 = [clean_markup(str(x)) for x in arr if str(x).strip()]\n",
    "            if arr2:\n",
    "                head = \", \".join(arr2[:6])\n",
    "                if len(arr2) > 6:\n",
    "                    head += f\", …(+{len(arr2)-6})\"\n",
    "                parts.append(f\"{label}: {head}\")\n",
    "\n",
    "    return \" | \".join(parts)\n",
    "\n",
    "def render_item(item: Any) -> str:\n",
    "    # Rend un 'item' (souvent dict avec value/meta/reference) en texte\n",
    "    if item is None:\n",
    "        return \"\"\n",
    "    if isinstance(item, str):\n",
    "        return clean_markup(item)\n",
    "\n",
    "    if isinstance(item, dict):\n",
    "        val = item.get(\"value\")\n",
    "        if val is None:\n",
    "            for k in (\"mainTitle\", \"title\", \"name\"):\n",
    "                if k in item:\n",
    "                    val = item.get(k)\n",
    "                    break\n",
    "\n",
    "        out = clean_markup(str(val)) if val is not None else \"\"\n",
    "\n",
    "        if INCLUDE_META and isinstance(item.get(\"meta\"), dict) and item[\"meta\"]:\n",
    "            m = summarize_meta(item[\"meta\"])\n",
    "            if m:\n",
    "                out = f\"{out} ({m})\" if out else f\"({m})\"\n",
    "\n",
    "        if INCLUDE_SOURCES and item.get(\"reference\"):\n",
    "            ref = item[\"reference\"]\n",
    "            if isinstance(ref, list):\n",
    "                ref = \" ; \".join([clean_markup(str(x)) for x in ref if str(x).strip()])\n",
    "            else:\n",
    "                ref = clean_markup(str(ref))\n",
    "            if ref:\n",
    "                out = f\"{out} [source: {ref}]\" if out else f\"[source: {ref}]\"\n",
    "\n",
    "        return out.strip()\n",
    "\n",
    "    if isinstance(item, list):\n",
    "        return \" | \".join([x for x in (render_item(x) for x in item) if x])\n",
    "\n",
    "    return clean_markup(str(item))\n",
    "\n",
    "def render_field(field_name: str, field_value: Any) -> List[str]:\n",
    "    # Rend un champ (souvent liste) en lignes de texte\n",
    "    lines: List[str] = []\n",
    "    if field_value is None:\n",
    "        return lines\n",
    "\n",
    "    if isinstance(field_value, list):\n",
    "        vals = [render_item(x) for x in field_value]\n",
    "        vals = [v for v in vals if v]\n",
    "        if vals:\n",
    "            if len(vals) == 1:\n",
    "                lines.append(f\"- {field_name}: {vals[0]}\")\n",
    "            else:\n",
    "                lines.append(f\"- {field_name}:\")\n",
    "                for v in vals:\n",
    "                    lines.append(f\"  - {v}\")\n",
    "        return lines\n",
    "\n",
    "    v = render_item(field_value)\n",
    "    if v:\n",
    "        lines.append(f\"- {field_name}: {v}\")\n",
    "    return lines\n",
    "\n",
    "def render_generic_section(section_name: str, section_obj: Any) -> List[str]:\n",
    "    # Rendu générique d'une section top-level (dict de champs)\n",
    "    lines: List[str] = [f\"[{section_name}]\"]\n",
    "    if isinstance(section_obj, dict):\n",
    "        for k, v in section_obj.items():\n",
    "            lines.extend(render_field(k, v))\n",
    "    elif isinstance(section_obj, list):\n",
    "        for it in section_obj:\n",
    "            v = render_item(it)\n",
    "            if v:\n",
    "                lines.append(f\"- {v}\")\n",
    "    else:\n",
    "        v = render_item(section_obj)\n",
    "        if v:\n",
    "            lines.append(f\"- {v}\")\n",
    "    return lines\n",
    "\n",
    "def render_textual_production(tp: Dict[str, Any]) -> List[List[str]]:\n",
    "    # Rend textualProduction en blocs (1 bloc ~ 1 oeuvre) pour chunker facilement\n",
    "    blocks: List[List[str]] = []\n",
    "    if not isinstance(tp, dict):\n",
    "        return blocks\n",
    "\n",
    "    for category, cat_obj in tp.items():\n",
    "        if not isinstance(cat_obj, dict):\n",
    "            continue\n",
    "        opus = cat_obj.get(\"opus\", [])\n",
    "        if not isinstance(opus, list) or not opus:\n",
    "            continue\n",
    "\n",
    "        for work in opus:\n",
    "            lines: List[str] = [f\"[textualProduction] category: {clean_markup(category)}\"]\n",
    "            if isinstance(work, dict):\n",
    "                main_title = work.get(\"mainTitle\")\n",
    "                if main_title:\n",
    "                    lines.append(f\"- title: {clean_markup(str(main_title))}\")\n",
    "                elif \"title\" in work:\n",
    "                    title_vals = render_item(work.get(\"title\"))\n",
    "                    if title_vals:\n",
    "                        lines.append(f\"- title: {title_vals}\")\n",
    "\n",
    "                for k, v in work.items():\n",
    "                    if k in (\"mainTitle\", \"title\"):\n",
    "                        continue\n",
    "                    if v is None:\n",
    "                        continue\n",
    "                    extra_lines = render_field(k, v)\n",
    "                    if extra_lines:\n",
    "                        lines.extend(extra_lines)\n",
    "\n",
    "            blocks.append(lines)\n",
    "\n",
    "    return blocks\n",
    "\n",
    "def detect_person_name(rec: Dict[str, Any]) -> str:\n",
    "    # Nom principal: identity.name[0].value sinon title\n",
    "    try:\n",
    "        name_list = rec.get(\"identity\", {}).get(\"name\", [])\n",
    "        if isinstance(name_list, list) and name_list:\n",
    "            nm = render_item(name_list[0])\n",
    "            if nm:\n",
    "                return nm\n",
    "    except Exception:\n",
    "        pass\n",
    "    return clean_markup(str(rec.get(\"title\", \"\"))).strip()\n",
    "\n",
    "def build_full_url(rec: Dict[str, Any]) -> str:\n",
    "    link = rec.get(\"link\", \"\") or \"\"\n",
    "    link = str(link)\n",
    "    if link.startswith(\"http://\") or link.startswith(\"https://\"):\n",
    "        return link\n",
    "    if link.startswith(\"/\"):\n",
    "        return BASE_URL.rstrip(\"/\") + link\n",
    "    if link:\n",
    "        return BASE_URL.rstrip(\"/\") + \"/\" + link\n",
    "    return BASE_URL\n",
    "\n",
    "def chunk_lines(header_lines: List[str], body_lines: List[str], max_chars: int) -> List[str]:\n",
    "    # Chunk un ensemble de lignes en plusieurs textes (<= max_chars), header répété à chaque chunk\n",
    "    chunks: List[str] = []\n",
    "    header = \"\\n\".join(header_lines).strip() + \"\\n\"\n",
    "    base_len = len(header)\n",
    "\n",
    "    current_lines: List[str] = []\n",
    "    current_len = base_len\n",
    "\n",
    "    def flush():\n",
    "        nonlocal current_lines, current_len\n",
    "        if current_lines:\n",
    "            chunks.append(header + \"\\n\".join(current_lines).strip() + \"\\n\")\n",
    "            current_lines = []\n",
    "            current_len = base_len\n",
    "\n",
    "    for line in body_lines:\n",
    "        if len(line) > max_chars - base_len - 10:\n",
    "            line = line[: max(0, max_chars - base_len - 10)] + \"…\"\n",
    "\n",
    "        add_len = len(line) + 1\n",
    "        if current_lines and (current_len + add_len > max_chars):\n",
    "            flush()\n",
    "\n",
    "        current_lines.append(line)\n",
    "        current_len += add_len\n",
    "\n",
    "    flush()\n",
    "    return chunks\n",
    "\n",
    "def flatten_paths(obj: Any, prefix: str = \"\") -> Set[str]:\n",
    "    # Retourne l'ensemble des chemins présents. Les listes sont notées avec [] (sans index).\n",
    "    paths: Set[str] = set()\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            p = f\"{prefix}.{k}\" if prefix else str(k)\n",
    "            paths.add(p)\n",
    "            paths |= flatten_paths(v, p)\n",
    "        return paths\n",
    "\n",
    "    if isinstance(obj, list):\n",
    "        p = f\"{prefix}[]\" if prefix else \"[]\"\n",
    "        paths.add(p)\n",
    "        for it in obj:\n",
    "            paths |= flatten_paths(it, p)\n",
    "        return paths\n",
    "\n",
    "    if prefix:\n",
    "        paths.add(prefix)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e14bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a1d15f8fad449fb0c8efc88f0884b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scanning JSONL: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records: 20149\n",
      "Unique top-level keys: 21\n",
      "Unique paths: 7159\n"
     ]
    }
   ],
   "source": [
    "# --- Étape 1 : Scanner le JSONL (inventaire sections & attributs) ---\n",
    "\n",
    "assert os.path.exists(INPUT_JSONL), (\n",
    "    f\"Fichier introuvable: {INPUT_JSONL}\\n\"\n",
    "    \"Mets le JSONL dans le même dossier que ce notebook ou modifie INPUT_JSONL.\"\n",
    ")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "top_level_counts = Counter()\n",
    "path_counts = Counter()\n",
    "total_records = 0\n",
    "\n",
    "with open(INPUT_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"Scanning JSONL\"):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            rec = json.loads(line)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        total_records += 1\n",
    "\n",
    "        # top-level keys (présence par fiche)\n",
    "        for k in set(rec.keys()):\n",
    "            top_level_counts[k] += 1\n",
    "\n",
    "        # chemins (présence par fiche)\n",
    "        rec_paths = flatten_paths(rec)\n",
    "        for p in rec_paths:\n",
    "            path_counts[p] += 1\n",
    "\n",
    "print(\"Records:\", total_records)\n",
    "print(\"Unique top-level keys:\", len(top_level_counts))\n",
    "print(\"Unique paths:\", len(path_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f7a54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: llm_ready_outputs\\studium_top_level_sections.csv\n",
      "✅ Saved: llm_ready_outputs\\studium_field_paths_coverage.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>count</th>\n",
       "      <th>coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_id</td>\n",
       "      <td>20149</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>extras</td>\n",
       "      <td>20149</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>identity</td>\n",
       "      <td>20149</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>link</td>\n",
       "      <td>20149</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>raw</td>\n",
       "      <td>20149</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>reference</td>\n",
       "      <td>20149</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>title</td>\n",
       "      <td>20149</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bibliography</td>\n",
       "      <td>20127</td>\n",
       "      <td>0.998908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>curriculum</td>\n",
       "      <td>19614</td>\n",
       "      <td>0.973448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>origin</td>\n",
       "      <td>11193</td>\n",
       "      <td>0.555511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ecclesiasticalCareer</td>\n",
       "      <td>10924</td>\n",
       "      <td>0.542161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>professionalCareer</td>\n",
       "      <td>6740</td>\n",
       "      <td>0.334508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>relationalInsertion</td>\n",
       "      <td>6257</td>\n",
       "      <td>0.310537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>textualProduction</td>\n",
       "      <td>1562</td>\n",
       "      <td>0.077522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>assets</td>\n",
       "      <td>1361</td>\n",
       "      <td>0.067547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     key  count  coverage\n",
       "0                    _id  20149  1.000000\n",
       "1                 extras  20149  1.000000\n",
       "2               identity  20149  1.000000\n",
       "3                   link  20149  1.000000\n",
       "4                    raw  20149  1.000000\n",
       "5              reference  20149  1.000000\n",
       "6                  title  20149  1.000000\n",
       "7           bibliography  20127  0.998908\n",
       "8             curriculum  19614  0.973448\n",
       "9                 origin  11193  0.555511\n",
       "10  ecclesiasticalCareer  10924  0.542161\n",
       "11    professionalCareer   6740  0.334508\n",
       "12   relationalInsertion   6257  0.310537\n",
       "13     textualProduction   1562  0.077522\n",
       "14                assets   1361  0.067547"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Export CSV : couverture des sections & des chemins ---\n",
    "\n",
    "top_level_df = (\n",
    "    pd.DataFrame(\n",
    "        [{\"key\": k, \"count\": int(c), \"coverage\": c / total_records} for k, c in top_level_counts.most_common()]\n",
    "    )\n",
    "    .sort_values([\"coverage\", \"key\"], ascending=[False, True])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "paths_df = (\n",
    "    pd.DataFrame(\n",
    "        [{\"path\": p, \"count\": int(c), \"coverage\": c / total_records} for p, c in path_counts.most_common()]\n",
    "    )\n",
    "    .sort_values([\"coverage\", \"path\"], ascending=[False, True])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "top_level_csv = os.path.join(OUTPUT_DIR, \"studium_top_level_sections.csv\")\n",
    "paths_csv = os.path.join(OUTPUT_DIR, \"studium_field_paths_coverage.csv\")\n",
    "\n",
    "top_level_df.to_csv(top_level_csv, index=False)\n",
    "paths_df.to_csv(paths_csv, index=False)\n",
    "\n",
    "print(\"Saved:\", top_level_csv)\n",
    "print(\"Saved:\", paths_csv)\n",
    "\n",
    "display(top_level_df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4b889fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Étape 2 : Conversion JSON -> texte chunké (LLM-ready) ---\n",
    "\n",
    "SECTION_TO_DOCTYPE = {\n",
    "    \"bibliography\": \"bibliography\",\n",
    "    \"textualProduction\": \"works\",\n",
    "}\n",
    "\n",
    "def record_to_llm_chunks(rec: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    ref = str(rec.get(\"reference\", \"\")).strip()\n",
    "    name = detect_person_name(rec)\n",
    "    url = build_full_url(rec)\n",
    "\n",
    "    header_lines = [\n",
    "        f\"person_reference: {ref}\",\n",
    "        f\"person_name: {name}\",\n",
    "        f\"url: {url}\",\n",
    "    ]\n",
    "\n",
    "    # --- BIO = toutes sections sauf bibliography/textualProduction + skip ---\n",
    "    bio_sections: List[str] = []\n",
    "\n",
    "    all_section_keys = [k for k in rec.keys() if k not in SKIP_TOP_LEVEL_KEYS]\n",
    "    all_section_keys = [k for k in all_section_keys if k not in (\"reference\", \"title\", \"link\")]\n",
    "\n",
    "    seen = set()\n",
    "    for k in BIO_SECTION_ORDER:\n",
    "        if k in rec and k not in SECTION_TO_DOCTYPE:\n",
    "            bio_sections.append(k)\n",
    "            seen.add(k)\n",
    "    for k in all_section_keys:\n",
    "        if k not in seen and k not in SECTION_TO_DOCTYPE and k not in (\"_id\",):\n",
    "            bio_sections.append(k)\n",
    "            seen.add(k)\n",
    "\n",
    "    bio_lines: List[str] = []\n",
    "    for sec in bio_sections:\n",
    "        sec_obj = rec.get(sec)\n",
    "        if sec_obj is None:\n",
    "            continue\n",
    "        bio_lines.extend(render_generic_section(sec, sec_obj))\n",
    "        bio_lines.append(\"\")\n",
    "\n",
    "    chunks_out: List[Dict[str, Any]] = []\n",
    "\n",
    "    # chunk bio\n",
    "    if bio_lines:\n",
    "        bio_texts = chunk_lines(header_lines, bio_lines, MAX_CHARS)\n",
    "        for i, txt in enumerate(bio_texts, start=1):\n",
    "            chunks_out.append({\n",
    "                \"chunk_id\": f\"{ref}_bio_{i:04d}\",\n",
    "                \"person_reference\": ref,\n",
    "                \"person_name\": name,\n",
    "                \"url\": url,\n",
    "                \"doc_type\": \"bio\",\n",
    "                \"text\": txt,\n",
    "            })\n",
    "\n",
    "    # bibliography\n",
    "    if \"bibliography\" in rec and isinstance(rec.get(\"bibliography\"), dict):\n",
    "        biblio_lines = render_generic_section(\"bibliography\", rec[\"bibliography\"])\n",
    "        biblio_texts = chunk_lines(header_lines, biblio_lines, MAX_CHARS)\n",
    "        for i, txt in enumerate(biblio_texts, start=1):\n",
    "            chunks_out.append({\n",
    "                \"chunk_id\": f\"{ref}_bibliography_{i:04d}\",\n",
    "                \"person_reference\": ref,\n",
    "                \"person_name\": name,\n",
    "                \"url\": url,\n",
    "                \"doc_type\": \"bibliography\",\n",
    "                \"text\": txt,\n",
    "            })\n",
    "\n",
    "    # works (textualProduction)\n",
    "    if \"textualProduction\" in rec and isinstance(rec.get(\"textualProduction\"), dict):\n",
    "        blocks = render_textual_production(rec[\"textualProduction\"])\n",
    "\n",
    "        header = \"\\n\".join(header_lines).strip() + \"\\n\"\n",
    "        base_len = len(header)\n",
    "        work_body_lines: List[str] = []\n",
    "        work_chunks: List[str] = []\n",
    "        cur_len = base_len\n",
    "\n",
    "        def flush_work():\n",
    "            nonlocal work_body_lines, work_chunks, cur_len\n",
    "            if work_body_lines:\n",
    "                work_chunks.append(header + \"\\n\".join(work_body_lines).strip() + \"\\n\")\n",
    "                work_body_lines = []\n",
    "                cur_len = base_len\n",
    "\n",
    "        for block in blocks:\n",
    "            block_lines = block + [\"\"]\n",
    "            block_len = sum(len(x) + 1 for x in block_lines)\n",
    "\n",
    "            if work_body_lines and (cur_len + block_len > MAX_CHARS):\n",
    "                flush_work()\n",
    "\n",
    "            # si bloc énorme et chunk vide, chunker direct par lignes\n",
    "            if block_len > MAX_CHARS - base_len and not work_body_lines:\n",
    "                direct = chunk_lines(header_lines, block, MAX_CHARS)\n",
    "                work_chunks.extend(direct)\n",
    "                continue\n",
    "\n",
    "            work_body_lines.extend(block_lines)\n",
    "            cur_len += block_len\n",
    "\n",
    "        flush_work()\n",
    "\n",
    "        for i, txt in enumerate(work_chunks, start=1):\n",
    "            chunks_out.append({\n",
    "                \"chunk_id\": f\"{ref}_works_{i:04d}\",\n",
    "                \"person_reference\": ref,\n",
    "                \"person_name\": name,\n",
    "                \"url\": url,\n",
    "                \"doc_type\": \"works\",\n",
    "                \"text\": txt,\n",
    "            })\n",
    "\n",
    "    return chunks_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b16fcd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7458264bdefe417e92cc9db636990975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building LLM-ready chunks: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Records processed: 20149\n",
      "✅ Chunks written: 47440\n",
      "✅ Output: llm_ready_outputs\\studium_llm_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# --- Génération du JSONL chunké (LLM-ready) ---\n",
    "\n",
    "out_jsonl = os.path.join(OUTPUT_DIR, \"studium_llm_chunks.jsonl\")\n",
    "\n",
    "n_chunks = 0\n",
    "n_records = 0\n",
    "\n",
    "with open(INPUT_JSONL, \"r\", encoding=\"utf-8\") as f_in, open(out_jsonl, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for line in tqdm(f_in, desc=\"Building LLM-ready chunks\"):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            rec = json.loads(line)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        n_records += 1\n",
    "        chunks = record_to_llm_chunks(rec)\n",
    "\n",
    "        for ch in chunks:\n",
    "            f_out.write(json.dumps(ch, ensure_ascii=False) + \"\\n\")\n",
    "            n_chunks += 1\n",
    "\n",
    "print(f\"✅ Done. Records processed: {n_records}\")\n",
    "print(f\"✅ Chunks written: {n_chunks}\")\n",
    "print(\"✅ Output:\", out_jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19a8beea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "chunk_id: 15657_bio_0001 | doc_type: bio\n",
      "person_reference: 15657\n",
      "person_name: ANCELINUS Galli\n",
      "url: http://studium-parisiense.univ-paris1.fr/individus/15657-ancelinusgalli\n",
      "[identity]\n",
      "- name: ANCELINUS Galli\n",
      "- nameVariant: Anselinus GALLI (noms: Anselinus GALLI)\n",
      "- shortDescription: Bachelier Décret\n",
      "- datesOfActivity: 1435-1435 (dates: 1435-1435)\n",
      "- gender: male\n",
      "- status: Maître\n",
      "\n",
      "[curriculum]\n",
      "- university: Paris 1435-1435. (dates: 1435-1435 | lieux: Paris)\n",
      "- grades:\n",
      "  - Maître ès arts (?Paris) :1435 ; (dates: ≤1435 | lieux: Paris)\n",
      "  - Bachelier en décret (Paris) en 1435 (9 avril) ; (dates: 1435 | lieux: Paris)\n",
      " ...\n",
      "\n",
      "================================================================================\n",
      "chunk_id: 15657_bibliography_0001 | doc_type: bibliography\n",
      "person_reference: 15657\n",
      "person_name: ANCELINUS Galli\n",
      "url: http://studium-parisiense.univ-paris1.fr/individus/15657-ancelinusgalli\n",
      "[bibliography]\n",
      "- workReferences: FOURNIER: 2, 5 ;\n",
      "- otherBases: STUDIUM : http://lamop-vs3.univ-paris1.fr/studium/ : Rédaction : Anne Tournieroux ; révision, Jean-Philippe Genet, 23/07/2020.\n",
      " ...\n",
      "\n",
      "================================================================================\n",
      "chunk_id: 948_bio_0001 | doc_type: bio\n",
      "person_reference: 948\n",
      "person_name: ARNALDUS Dordraci\n",
      "url: http://studium-parisiense.univ-paris1.fr/individus/948-arnaldusdordraci\n",
      "[identity]\n",
      "- name: ARNALDUS Dordraci\n",
      "- nameVariant:\n",
      "  - Arnoldus DORDRACI (noms: Arnoldus DORDRACI)\n",
      "  - Arnoldus DURDRATI (noms: Arnoldus DURDRATI)\n",
      "- shortDescription: Maître ès arts\n",
      "- datesOfActivity: 1485-1486 (dates: 1485-1486)\n",
      "- activityMediane: 1486 (dates: 1486)\n",
      "- gender: male\n",
      "- status: Maître\n",
      "\n",
      "[origin]\n",
      "- birthPlace: Pays-Bas (Hollande : Dordrecht ?). (lieux: Pays-Bas, Hollande, Dordrecht)\n",
      "- diocese: Diocèse d'Utrecht.\n",
      "\n",
      "[curriculum]\n",
      "- university:\n",
      "  - Paris (Nation Anglo-allemande) :1485 ; (dates: ≤1485 | lieux: Paris)\n",
      "  - Padoue 1491 ; (dates: 1491 | lieux: Padoue)\n",
      "  - Ferrare 1491. (dates: 1491 | lieux: Ferrare)\n",
      "- grades:\n",
      "  - Bachelier ès arts (Paris) 1485 ; (dates: 1485 | lieux: Paris)\n",
      "  - Licencié ès arts (Paris) 1486. (dates: 1486 | lieux: Paris)\n",
      "  - Maître ès arts (Paris) 1486 ; (dates: 1486 | lieux: Paris)\n",
      "  - Docteur en médecine (Ferrare) 1491 ; (dates: 1491 | lieux: Ferrare)\n",
      " ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Vérification rapide : afficher quelques chunks ---\n",
    "\n",
    "import itertools\n",
    "\n",
    "with open(out_jsonl, \"r\", encoding=\"utf-8\") as f:\n",
    "    sample = list(itertools.islice(f, 3))\n",
    "\n",
    "for i, line in enumerate(sample, start=1):\n",
    "    obj = json.loads(line)\n",
    "    print(\"=\"*80)\n",
    "    print(\"chunk_id:\", obj[\"chunk_id\"], \"| doc_type:\", obj[\"doc_type\"])\n",
    "    print(obj[\"text\"][:1200], \"...\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
